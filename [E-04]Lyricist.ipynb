{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72e5054f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n"
     ]
    }
   ],
   "source": [
    "import os, re \n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담기\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66dc28bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====데이터 전처리====\n",
    "\n",
    "# 정규화를 이용한 정제 함수\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) \n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) \n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "corpus = []\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "\n",
    "corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f8d200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus를 텐서로 변환\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # tokenizer 내부의 단어장을 완성\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # corpus -> tensor 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    #print(len(tensor))\n",
    "    # 길이가 15이상이면 제외\n",
    "    for index, value in enumerate(tensor):\n",
    "        if len(tensor[index]) >= 15 :\n",
    "            del tensor[index]\n",
    "    #print(len(tensor))\n",
    "    # 시퀀스 데이터 조정\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "\n",
    "# 소스와 타켓으로 분리\n",
    "src_input = tensor[:, :-1]  \n",
    "tgt_input = tensor[:, 1:]  \n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3879566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((100, 346), (100, 346)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus 텐서를 데이터셋 객체로 변환\n",
    "# src_input->enc_train\n",
    "# tgt_input->dec_train\n",
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로 데이터셋을 만듦\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) \n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a574e212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 346, 12001), dtype=float32, numpy=\n",
       "array([[[ 8.10096972e-05,  1.00796851e-05,  1.16046045e-04, ...,\n",
       "         -7.68064274e-06, -2.66981988e-05, -7.07694635e-05],\n",
       "        [ 1.46241131e-04,  8.66715927e-05,  2.58704618e-04, ...,\n",
       "         -2.69411667e-05, -8.50036158e-05, -1.21281926e-04],\n",
       "        [ 7.58928436e-05,  5.89185765e-06,  2.98581173e-04, ...,\n",
       "         -4.92488180e-05, -7.60763869e-05, -1.99682501e-04],\n",
       "        ...,\n",
       "        [-1.00838079e-03,  3.65082378e-04,  2.99721374e-04, ...,\n",
       "          1.04058860e-03,  2.34526768e-03,  1.00636412e-03],\n",
       "        [-1.00838090e-03,  3.65082349e-04,  2.99721316e-04, ...,\n",
       "          1.04058860e-03,  2.34526745e-03,  1.00636424e-03],\n",
       "        [-1.00838102e-03,  3.65082233e-04,  2.99721432e-04, ...,\n",
       "          1.04058860e-03,  2.34526745e-03,  1.00636424e-03]],\n",
       "\n",
       "       [[ 8.10096972e-05,  1.00796851e-05,  1.16046045e-04, ...,\n",
       "         -7.68064274e-06, -2.66981988e-05, -7.07694635e-05],\n",
       "        [ 5.48396929e-05,  8.36044364e-06,  1.16105854e-04, ...,\n",
       "          3.69995105e-05,  6.90611341e-05, -1.01858335e-04],\n",
       "        [ 4.81471725e-05,  1.76998255e-05,  2.39276796e-05, ...,\n",
       "          9.63485072e-05,  1.11261623e-04, -1.19760189e-04],\n",
       "        ...,\n",
       "        [-1.00838044e-03,  3.65082087e-04,  2.99721607e-04, ...,\n",
       "          1.04058825e-03,  2.34526768e-03,  1.00636459e-03],\n",
       "        [-1.00838067e-03,  3.65082087e-04,  2.99721607e-04, ...,\n",
       "          1.04058837e-03,  2.34526768e-03,  1.00636459e-03],\n",
       "        [-1.00838055e-03,  3.65082029e-04,  2.99721607e-04, ...,\n",
       "          1.04058825e-03,  2.34526768e-03,  1.00636447e-03]],\n",
       "\n",
       "       [[ 8.10096972e-05,  1.00796851e-05,  1.16046045e-04, ...,\n",
       "         -7.68064274e-06, -2.66981988e-05, -7.07694635e-05],\n",
       "        [ 1.90778810e-04, -5.78018189e-05,  1.64907018e-04, ...,\n",
       "         -7.92040373e-05, -6.30123759e-05, -1.20356184e-04],\n",
       "        [ 2.68819014e-04, -1.68860017e-04,  2.14348460e-04, ...,\n",
       "         -1.98973066e-04, -1.99801234e-05, -2.67768395e-04],\n",
       "        ...,\n",
       "        [-1.00838090e-03,  3.65081913e-04,  2.99721258e-04, ...,\n",
       "          1.04058848e-03,  2.34526698e-03,  1.00636377e-03],\n",
       "        [-1.00838090e-03,  3.65082000e-04,  2.99721200e-04, ...,\n",
       "          1.04058848e-03,  2.34526698e-03,  1.00636377e-03],\n",
       "        [-1.00838090e-03,  3.65082000e-04,  2.99721258e-04, ...,\n",
       "          1.04058848e-03,  2.34526722e-03,  1.00636377e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 8.10096972e-05,  1.00796851e-05,  1.16046045e-04, ...,\n",
       "         -7.68064274e-06, -2.66981988e-05, -7.07694635e-05],\n",
       "        [ 2.74946855e-04,  6.08418759e-07,  2.55395949e-04, ...,\n",
       "         -1.41601195e-04, -4.88957667e-05, -9.38988160e-05],\n",
       "        [ 4.02018020e-04, -1.21793739e-04,  2.78251129e-04, ...,\n",
       "         -2.19566544e-04, -4.11296496e-05, -1.18367745e-04],\n",
       "        ...,\n",
       "        [-1.00838114e-03,  3.65082262e-04,  2.99720909e-04, ...,\n",
       "          1.04058895e-03,  2.34526768e-03,  1.00636366e-03],\n",
       "        [-1.00838090e-03,  3.65082291e-04,  2.99720909e-04, ...,\n",
       "          1.04058895e-03,  2.34526745e-03,  1.00636366e-03],\n",
       "        [-1.00838114e-03,  3.65082291e-04,  2.99720909e-04, ...,\n",
       "          1.04058895e-03,  2.34526745e-03,  1.00636366e-03]],\n",
       "\n",
       "       [[ 8.10096972e-05,  1.00796851e-05,  1.16046045e-04, ...,\n",
       "         -7.68064274e-06, -2.66981988e-05, -7.07694635e-05],\n",
       "        [ 5.48396929e-05,  8.36044364e-06,  1.16105854e-04, ...,\n",
       "          3.69995105e-05,  6.90611341e-05, -1.01858335e-04],\n",
       "        [ 1.02131009e-04, -5.27480515e-05,  1.39252559e-04, ...,\n",
       "          6.90855959e-05,  5.44164868e-05, -9.72303678e-05],\n",
       "        ...,\n",
       "        [-1.00838067e-03,  3.65082320e-04,  2.99721170e-04, ...,\n",
       "          1.04058848e-03,  2.34526768e-03,  1.00636366e-03],\n",
       "        [-1.00838067e-03,  3.65082349e-04,  2.99721170e-04, ...,\n",
       "          1.04058848e-03,  2.34526768e-03,  1.00636366e-03],\n",
       "        [-1.00838067e-03,  3.65082349e-04,  2.99721170e-04, ...,\n",
       "          1.04058848e-03,  2.34526768e-03,  1.00636366e-03]],\n",
       "\n",
       "       [[ 8.10096972e-05,  1.00796851e-05,  1.16046045e-04, ...,\n",
       "         -7.68064274e-06, -2.66981988e-05, -7.07694635e-05],\n",
       "        [ 1.62618875e-04,  6.48932692e-05,  2.30884019e-04, ...,\n",
       "         -2.48851620e-06, -7.12046358e-06, -4.82811593e-05],\n",
       "        [ 1.13780006e-04,  1.17938856e-04,  4.59491741e-04, ...,\n",
       "         -5.45504481e-05, -7.15563656e-05, -9.76744741e-06],\n",
       "        ...,\n",
       "        [-1.00838067e-03,  3.65081767e-04,  2.99721287e-04, ...,\n",
       "          1.04058837e-03,  2.34526792e-03,  1.00636331e-03],\n",
       "        [-1.00838079e-03,  3.65081767e-04,  2.99721345e-04, ...,\n",
       "          1.04058837e-03,  2.34526792e-03,  1.00636331e-03],\n",
       "        [-1.00838079e-03,  3.65081825e-04,  2.99721345e-04, ...,\n",
       "          1.04058837e-03,  2.34526792e-03,  1.00636331e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습시키기\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 50\n",
    "hidden_size = 300\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3d5cac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      multiple                  600050    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                multiple                  421200    \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                multiple                  721200    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  3612301   \n",
      "=================================================================\n",
      "Total params: 5,354,751\n",
      "Trainable params: 5,354,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "066c8f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1241/1241 [==============================] - 637s 512ms/step - loss: 0.2272\n",
      "Epoch 2/10\n",
      "1241/1241 [==============================] - 642s 517ms/step - loss: 0.1341\n",
      "Epoch 3/10\n",
      "1241/1241 [==============================] - 644s 519ms/step - loss: 0.1274\n",
      "Epoch 4/10\n",
      "1241/1241 [==============================] - 646s 521ms/step - loss: 0.1228\n",
      "Epoch 5/10\n",
      "1241/1241 [==============================] - 650s 523ms/step - loss: 0.1282\n",
      "Epoch 6/10\n",
      "1241/1241 [==============================] - 650s 524ms/step - loss: 0.1185\n",
      "Epoch 7/10\n",
      "1241/1241 [==============================] - 651s 524ms/step - loss: 0.1150\n",
      "Epoch 8/10\n",
      "1241/1241 [==============================] - 651s 524ms/step - loss: 0.1116\n",
      "Epoch 9/10\n",
      "1241/1241 [==============================] - 651s 525ms/step - loss: 0.1083\n",
      "Epoch 10/10\n",
      "1241/1241 [==============================] - 652s 525ms/step - loss: 0.1050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe583faf850>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ce3ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6500210a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i call you in the world <end> '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i call\", max_len=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
